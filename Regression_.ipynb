{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.What is Simple Linear Regression?\n",
        "It’s a statistical method that models the relationship between a dependent variable (Y) and a single independent variable (X) using a straight line: Y = mX + c.\n",
        "\n",
        "2.What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Linearity: Relationship between X and Y is linear.\n",
        "\n",
        "Independence: Observations are independent.\n",
        "\n",
        "Homoscedasticity: Constant variance of residuals.\n",
        "\n",
        "Normality: Residuals are normally distributed.\n",
        "\n",
        "3.What does the coefficient m represent in the equation Y = mX + c?\n",
        "The slope – it shows the change in Y for a one-unit change in X.\n",
        "\n",
        "4.What does the intercept c represent in the equation Y = mX + c?\n",
        "The value of Y when X = 0.\n",
        "\n",
        "5.How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "𝑚\n",
        "=\n",
        "𝑛\n",
        "(\n",
        "∑\n",
        "𝑋\n",
        "𝑌\n",
        ")\n",
        "−\n",
        "(\n",
        "∑\n",
        "𝑋\n",
        ")\n",
        "(\n",
        "∑\n",
        "𝑌\n",
        ")\n",
        "𝑛\n",
        "(\n",
        "∑\n",
        "𝑋\n",
        "2\n",
        ")\n",
        "−\n",
        "(\n",
        "∑\n",
        "𝑋\n",
        ")\n",
        "2\n",
        "m=\n",
        "n(∑X\n",
        "2\n",
        " )−(∑X)\n",
        "2\n",
        "\n",
        "n(∑XY)−(∑X)(∑Y)\n",
        "​\n",
        "\n",
        "6.What is the purpose of the least squares method in Simple Linear Regression?\n",
        "To minimize the sum of the squared residuals (differences between observed and predicted values).\n",
        "\n",
        "7.How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "It indicates the proportion of variance in Y explained by X. R² = 1 means perfect fit.\n",
        "\n",
        "Multiple Linear Regression\n",
        "8.What is Multiple Linear Regression?\n",
        "It models the relationship between a dependent variable and two or more independent variables.\n",
        "\n",
        "9.What is the main difference between Simple and Multiple Linear Regression?\n",
        "Simple uses one predictor variable, while multiple uses two or more.\n",
        "\n",
        "10.What are the key assumptions of Multiple Linear Regression?\n",
        "Same as simple regression, plus:\n",
        "\n",
        "No multicollinearity among predictors.\n",
        "\n",
        "Model is correctly specified.\n",
        "\n",
        "11.What is heteroscedasticity, and how does it affect results?\n",
        "Unequal variance of residuals; it can lead to inefficient estimates and invalid statistical tests.\n",
        "\n",
        "12.How can you improve a model with high multicollinearity?\n",
        "\n",
        "Remove correlated predictors.\n",
        "\n",
        "Use Principal Component Analysis (PCA).\n",
        "\n",
        "Use regularization (Ridge/Lasso).\n",
        "\n",
        "Techniques for transforming categorical variables?\n",
        "\n",
        "One-hot encoding\n",
        "\n",
        "Label encoding\n",
        "\n",
        "Binary encoding\n",
        "\n",
        "13.What is the role of interaction terms in MLR?\n",
        "They show how the effect of one variable depends on the level of another.\n",
        "\n",
        "14.How can the interpretation of intercept differ between Simple and Multiple Regression?\n",
        "In simple: it’s Y when X=0. In multiple: it’s Y when all predictors = 0 (which may not be meaningful).\n",
        "\n",
        "15.What is the significance of the slope in regression, and how does it affect predictions?\n",
        "Each slope represents the change in Y per unit change in its corresponding X, assuming other variables are constant.\n",
        "\n",
        "16.How does the intercept provide context in regression?\n",
        "It sets the baseline value of Y when all Xs are 0.\n",
        "\n",
        "Limitations of using R² as sole model performance measure?\n",
        "\n",
        "Doesn’t indicate causality or model quality.\n",
        "\n",
        "Always increases with more predictors, even if they’re irrelevant.\n",
        "\n",
        "How would you interpret a large standard error for a regression coefficient?\n",
        "It suggests high variability or uncertainty in the estimate; may not be statistically significant.\n",
        "\n",
        "How can heteroscedasticity be identified and why address it?\n",
        "\n",
        "Residual plots (funnel shape)\n",
        "\n",
        "Breusch-Pagan test\n",
        "It can distort hypothesis tests and confidence intervals.\n",
        "\n",
        "What does it mean if MLR has high R² but low adjusted R²?\n",
        "Some predictors may not contribute meaningfully – adjusted R² penalizes for unnecessary variables.\n",
        "\n",
        "Why is scaling important in MLR?\n",
        "Especially for regularization or algorithms sensitive to scale (like gradient descent). It ensures equal weight and improves convergence.\n",
        "\n",
        "Polynomial Regression\n",
        "How does polynomial regression differ from linear regression?\n",
        "Linear: straight line. Polynomial: fits nonlinear relationships using higher-degree terms (e.g., X², X³).\n",
        "\n",
        "When is polynomial regression used?\n",
        "When the relationship between X and Y is nonlinear but continuous and smooth.\n",
        "\n",
        "General equation of polynomial regression?\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝑏\n",
        "3\n",
        "𝑋\n",
        "3\n",
        "+\n",
        ".\n",
        ".\n",
        ".\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "Y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " X+b\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +b\n",
        "3\n",
        "​\n",
        " X\n",
        "3\n",
        " +...+b\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "\n",
        "Can polynomial regression be applied to multiple variables?\n",
        "Yes, called multivariate polynomial regression.\n",
        "\n",
        "Limitations of polynomial regression?\n",
        "\n",
        "Overfitting\n",
        "\n",
        "Poor extrapolation\n",
        "\n",
        "Complexity increases with degree\n",
        "\n",
        "Methods to evaluate model fit for polynomial degree?\n",
        "\n",
        "Cross-validation\n",
        "\n",
        "Adjusted R²\n",
        "\n",
        "AIC/BIC\n",
        "\n",
        "Why is visualization important in polynomial regression?\n",
        "It helps understand overfitting, underfitting, and model behavior.\n",
        "\n",
        "How is polynomial regression implemented in Python?\n",
        "Using PolynomialFeatures from sklearn.preprocessing and then applying linear regression:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(PolynomialFeatures(degree=3), LinearRegression())\n",
        "model.fit(X, y)\n"
      ],
      "metadata": {
        "id": "MdPwWYSYLAxq"
      }
    }
  ]
}